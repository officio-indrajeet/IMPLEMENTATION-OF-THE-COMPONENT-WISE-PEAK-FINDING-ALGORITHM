{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040132ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Step 1: Build CCGraph\n",
      "Completed Step 1: Build CCGraph\n",
      "Step 1: Build CCGraph took 0.6775 seconds\n",
      "Starting Step 2: Get Density Dists BB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'density_broad_search' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'density_broad_search' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'density_broad_search' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'density_broad_search' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/indrajeet/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 304\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Measure time taken\u001b[39;00m\n\u001b[1;32m    303\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 304\u001b[0m cpf_model\u001b[38;5;241m.\u001b[39mfit(data_normalized)\n\u001b[1;32m    305\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    307\u001b[0m predicted_labels_cpf \u001b[38;5;241m=\u001b[39m cpf_model\u001b[38;5;241m.\u001b[39mlabels_\n",
      "Cell \u001b[0;32mIn[8], line 235\u001b[0m, in \u001b[0;36mCPFcluster.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Step 2: Get Density Dists BB\u001b[39;00m\n\u001b[1;32m    234\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 235\u001b[0m best_distance, big_brother \u001b[38;5;241m=\u001b[39m get_density_dists_bb(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents, knn_radius, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    236\u001b[0m step2_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 2: Get Density Dists BB took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep2_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 97\u001b[0m, in \u001b[0;36mget_density_dists_bb\u001b[0;34m(X, k, components, knn_radius, n_jobs)\u001b[0m\n\u001b[1;32m     95\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     distance_comp \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(density_broad_search, itertools\u001b[38;5;241m.\u001b[39mislice(GT_distances, N))\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_comp:\n\u001b[1;32m     99\u001b[0m         distances\u001b[38;5;241m.\u001b[39mappend(distance_comp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# utils module\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def density_broad_search(a_b):\n",
    "    \"\"\"Wrapper for multiprocessing to call density_broad_search.\"\"\"\n",
    "    try:\n",
    "        return euclidean_distances(a_b[1], a_b[0])\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    print(\"Starting Step 1: Build CCGraph\")\n",
    "    n = X.shape[0]\n",
    "    kdt = NearestNeighbors(n_neighbors=k, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X)\n",
    "    CCmat = kdt.kneighbors_graph(X, mode='distance')\n",
    "    distances, _ = kdt.kneighbors(X)\n",
    "    knn_radius = distances[:, k-1]\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    print(\"Completed Step 1: Build CCGraph\")\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    print(\"Starting Step 2: Get Density Dists BB\")\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    ps = np.zeros((1, 2))\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        kdt = NearestNeighbors(n_neighbors=kcc, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X[cc_idx, :])\n",
    "        distances, neighbors = kdt.kneighbors(X[cc_idx, :])\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        del cc_radius_diff\n",
    "        gc.collect()\n",
    "        cols = cols[unidx]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        search_idx = list(np.setdiff1d(list(range(X[cc_idx, :].shape[0])), rows))\n",
    "        ps = np.vstack((ps, [len(cc_idx), len(search_idx)/len(cc_idx)]))\n",
    "        for indx_chunk in chunks(search_idx, 100):\n",
    "            search_radius = cc_knn_radius[indx_chunk]\n",
    "            GT_radius = cc_knn_radius < search_radius[:, np.newaxis]\n",
    "            if any(np.sum(GT_radius, axis=1) == 0):\n",
    "                max_i = [i for i in range(GT_radius.shape[0]) if np.sum(GT_radius[i, :]) == 0]\n",
    "                if len(max_i) > 1:\n",
    "                    for max_j in max_i[1:len(max_i)]:\n",
    "                        GT_radius[max_j, indx_chunk[max_i[0]]] = True\n",
    "                max_i = max_i[0]\n",
    "                cc_big_brother[indx_chunk[max_i]] = indx_chunk[max_i]\n",
    "                cc_best_distance[indx_chunk[max_i]] = np.inf\n",
    "                del indx_chunk[max_i]\n",
    "                GT_radius = np.delete(GT_radius, max_i, 0)\n",
    "            \n",
    "            GT_distances = ([X[cc_idx[indx_chunk[i]], np.newaxis], X[cc_idx[GT_radius[i, :]], :]] for i in range(len(indx_chunk)))\n",
    "            if (GT_radius.shape[0] > 50):\n",
    "                try:\n",
    "                    pool = mp.Pool(processes=n_jobs)\n",
    "                    N = 25\n",
    "                    distances = []\n",
    "                    i = 0\n",
    "                    while True:\n",
    "                        distance_comp = pool.map(density_broad_search, itertools.islice(GT_distances, N))\n",
    "                        if distance_comp:\n",
    "                            distances.append(distance_comp)\n",
    "                            i += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    distances = [dis_pair for dis_list in distances for dis_pair in dis_list]\n",
    "                    argmin_distance = [np.argmin(l) for l in distances]\n",
    "                    pool.terminate()\n",
    "                except Exception as e:\n",
    "                    print(\"POOL ERROR: \" + str(e))\n",
    "                    pool.close()\n",
    "                    pool.terminate()\n",
    "            else:\n",
    "                distances = list(map(density_broad_search, list(GT_distances)))\n",
    "                argmin_distance = [np.argmin(l) for l in distances]\n",
    "            \n",
    "            for i in range(GT_radius.shape[0]):\n",
    "                cc_big_brother[indx_chunk[i]] = np.where(GT_radius[i, :] == 1)[0][argmin_distance[i]]\n",
    "                cc_best_distance[indx_chunk[i]] = distances[i][argmin_distance[i]]\n",
    "        \n",
    "        big_brother[cc_idx] = [cc_idx[i] for i in cc_big_brother.astype(int)]\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    print(\"Completed Step 2: Get Density Dists BB\")\n",
    "    return best_distance, big_brother\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    print(\"Starting Step 3: Get Y\")\n",
    "    n = components.shape[0]\n",
    "    y_pred = np.repeat(-1, n)\n",
    "    peaks = []\n",
    "    n_cent = 0\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        tested = []\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        index = np.argsort(cc_idx)\n",
    "        sorted_x = cc_idx[index]\n",
    "        sorted_index = np.searchsorted(sorted_x, big_brother[cc_idx])\n",
    "        cc_big_brother = np.take(index, sorted_index, mode=\"clip\")\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "        peaked = cc_best_distance / cc_knn_radius\n",
    "        peaked[(cc_best_distance == 0) * (cc_knn_radius == 0)] = np.inf\n",
    "        cc_centers = [np.argmax(peaked)]\n",
    "        not_tested[cc_centers[0]] = False\n",
    "        while True:\n",
    "            if np.sum(not_tested) == 0:\n",
    "                break\n",
    "            subset_idx = np.argmax(peaked[not_tested])\n",
    "            prop_cent = np.arange(peaked.shape[0])[not_tested][subset_idx]\n",
    "            tested.append(np.arange(peaked.shape[0])[not_tested][subset_idx])\n",
    "            CCmat_level = CCmat[cc_idx, :][:, cc_idx]\n",
    "            if cc_knn_radius[prop_cent] > max(cc_knn_radius[~not_tested]):\n",
    "                cc_level_set = np.where(cc_knn_radius <= cc_knn_radius[prop_cent])[0]\n",
    "                CCmat_check = CCmat_level[cc_level_set, :][:, cc_level_set]\n",
    "                n_cc, _ = scipy.sparse.csgraph.connected_components(CCmat_check, directed=False, return_labels=True)\n",
    "                if n_cc == 1:\n",
    "                    break\n",
    "            if cc_knn_radius[prop_cent] > 0:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) > e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius < v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            else:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) >= e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius <= v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            _, cc_labels = scipy.sparse.csgraph.connected_components(CCmat_level, directed=False, return_labels=True)\n",
    "            del CCmat_level\n",
    "            gc.collect()\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            prop_cent_comp = cc_labels[np.where(cc_cut_idx == prop_cent)[0]]\n",
    "            if np.isin(prop_cent_comp, center_comp):\n",
    "                if peaked[prop_cent] == min(peaked[cc_centers]):\n",
    "                    cc_centers.append(prop_cent)\n",
    "                    not_tested[prop_cent] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                cc_centers.append(prop_cent)\n",
    "                not_tested[prop_cent] = False\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = cc_big_brother\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    print(\"Completed Step 3: Get Y\")\n",
    "    return y_pred\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise ValueError(\"X must be an n x d numpy array.\")\n",
    "        \n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        # Step 1: Build CCGraph\n",
    "        start_time = time.time()\n",
    "        self.components, self.CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 2: Get Density Dists BB\n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, self.components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 3: Get Y\n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(self.CCmat, self.components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'dataset_02052023.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Remove target variables\n",
    "data = data.drop(columns=['Robot_ProtectiveStop', 'grip_lost'])\n",
    "\n",
    "# Remove unnecessary columns (unnamed columns in this case) and the Timestamp column\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "data = data.drop(columns=['Timestamp'])\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# One-hot encode categorical variables if any\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 7, size=data_normalized.shape[0])  # Assuming 7 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    # Removing NaN entries\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of CobotOps Data with CPF Clusters', 'pca_clusters_CPF.png')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdedfeb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

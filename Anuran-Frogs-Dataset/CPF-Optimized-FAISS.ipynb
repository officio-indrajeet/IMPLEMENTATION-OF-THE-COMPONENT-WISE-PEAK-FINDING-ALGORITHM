{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c774ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Build CCGraph took 0.8011 seconds\n",
      "Step 2: Get Density Dists BB took 0.1317 seconds\n",
      "Step 3: Get Y took 0.0146 seconds\n",
      "CPF Clustering:\n",
      "Number of clusters found: 449\n",
      "Total Time taken: 0.95 seconds\n",
      "Adjusted Rand Index (ARI): 0.000051\n",
      "Adjusted Mutual Information (AMI): 0.000043\n",
      "Silhouette Score: -0.302447\n",
      "Davies-Bouldin Index: 1.078080\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    n = X.shape[0]\n",
    "    kdt = NearestNeighbors(n_neighbors=k, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X)\n",
    "    CCmat = kdt.kneighbors_graph(X, mode='distance')\n",
    "    distances, _ = kdt.kneighbors(X)\n",
    "    knn_radius = distances[:, k-1]\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    \n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        kdt = NearestNeighbors(n_neighbors=kcc, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X[cc_idx, :])\n",
    "        distances, neighbors = kdt.kneighbors(X[cc_idx, :])\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        \n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        cols = cols[unidx]\n",
    "        \n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        \n",
    "        cc_big_brother[cc_big_brother >= len(cc_idx)] = len(cc_idx) - 1\n",
    "        cc_big_brother = cc_idx[cc_big_brother.astype(int)]\n",
    "        \n",
    "        big_brother[cc_idx] = cc_big_brother\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    return best_distance, big_brother\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    y_pred = np.empty((CCmat.shape[0]))\n",
    "    y_pred[:] = np.nan\n",
    "    n_cent = 0\n",
    "    peaks = []\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        if nc <= 2:\n",
    "            y_pred[cc_idx] = n_cent\n",
    "            n_cent += 1\n",
    "            continue\n",
    "\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        cc_centers = []\n",
    "\n",
    "        cc_cut_idx = np.where(knn_radius[cc_idx] >= (rho * max(knn_radius[cc_idx])))[0]\n",
    "        cc_centers.append(cc_cut_idx[np.argmax(cc_best_distance[cc_cut_idx])])\n",
    "\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "\n",
    "        while sum(not_tested) > 0:\n",
    "            prop_cent = np.argmax(cc_best_distance * not_tested)\n",
    "            if prop_cent not in cc_centers:\n",
    "                cc_centers.append(prop_cent)\n",
    "            not_tested[prop_cent] = False\n",
    "\n",
    "            if len(cc_centers) > 1:\n",
    "                min_knn_radius_center = np.argmin(knn_radius[cc_centers])\n",
    "                if knn_radius[prop_cent] == knn_radius[cc_centers[min_knn_radius_center]]:\n",
    "                    break\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = big_brother[cc_idx]\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "\n",
    "        BBTree[BBTree[:, 1] >= nc, 1] = nc - 1\n",
    "\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        components, CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(CCmat, components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Frogs_MFCCs.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and remove non-Likert scale attributes\n",
    "features = data.drop(columns=['Family', 'Genus', 'Species', 'RecordID'])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 10, size=data_normalized.shape[0])  # Assuming 10 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Frog MFCC Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b021633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Build CCGraph took 0.1422 seconds\n",
      "Step 2: Get Density Dists BB took 0.0256 seconds\n",
      "Step 3: Get Y took 0.0054 seconds\n",
      "Total Time taken: 0.1731 seconds\n",
      "CPF Clustering:\n",
      "Number of clusters found: 83\n",
      "Total Time taken: 0.18 seconds\n",
      "Adjusted Rand Index (ARI): -0.000209\n",
      "Adjusted Mutual Information (AMI): 0.000227\n",
      "Silhouette Score: -0.364881\n",
      "Davies-Bouldin Index: 3.186732\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "import scipy.sparse\n",
    "import time\n",
    "import faiss\n",
    "\n",
    "# Optimized CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    n = X.shape[0]\n",
    "    index = faiss.IndexFlatL2(X.shape[1])\n",
    "    index.add(X.astype(np.float32))\n",
    "    distances, indices = index.search(X.astype(np.float32), k)\n",
    "    knn_radius = distances[:, k-1]\n",
    "    CCmat = scipy.sparse.lil_matrix((n, n))\n",
    "    for i in range(n):\n",
    "        for j in indices[i, :]:\n",
    "            CCmat[i, j] = 1\n",
    "            CCmat[j, i] = 1\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    \n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        index = faiss.IndexFlatL2(X.shape[1])\n",
    "        index.add(X[cc_idx, :].astype(np.float32))\n",
    "        distances, neighbors = index.search(X[cc_idx, :].astype(np.float32), kcc)\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        \n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        cols = cols[unidx]\n",
    "        \n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        \n",
    "        cc_big_brother[cc_big_brother >= len(cc_idx)] = len(cc_idx) - 1\n",
    "        cc_big_brother = cc_idx[cc_big_brother.astype(int)]\n",
    "        \n",
    "        big_brother[cc_idx] = cc_big_brother\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    return best_distance, big_brother\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    y_pred = np.empty((CCmat.shape[0]))\n",
    "    y_pred[:] = np.nan\n",
    "    n_cent = 0\n",
    "    peaks = []\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        if nc <= 2:\n",
    "            y_pred[cc_idx] = n_cent\n",
    "            n_cent += 1\n",
    "            continue\n",
    "\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        cc_centers = []\n",
    "\n",
    "        cc_cut_idx = np.where(knn_radius[cc_idx] >= (rho * max(knn_radius[cc_idx])))[0]\n",
    "        cc_centers.append(cc_cut_idx[np.argmax(cc_best_distance[cc_cut_idx])])\n",
    "\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "\n",
    "        while sum(not_tested) > 0:\n",
    "            prop_cent = np.argmax(cc_best_distance * not_tested)\n",
    "            if prop_cent not in cc_centers:\n",
    "                cc_centers.append(prop_cent)\n",
    "            not_tested[prop_cent] = False\n",
    "\n",
    "            if len(cc_centers) > 1:\n",
    "                min_knn_radius_center = np.argmin(knn_radius[cc_centers])\n",
    "                if knn_radius[prop_cent] == knn_radius[cc_centers[min_knn_radius_center]]:\n",
    "                    break\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = big_brother[cc_idx]\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "\n",
    "        BBTree[BBTree[:, 1] >= nc, 1] = nc - 1\n",
    "\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "class CPFclusterOptimized:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        components, CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(CCmat, components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "        total_time = step1_time + step2_time + step3_time\n",
    "        print(f\"Total Time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Frogs_MFCCs.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and remove non-Likert scale attributes\n",
    "features = data.drop(columns=['Family', 'Genus', 'Species', 'RecordID'])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 10, size=data_normalized.shape[0])  # Assuming 10 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFclusterOptimized(k=10, rho=0.4, n_jobs=1)\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Frog MFCC Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a28191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 5325 7124 5754]\n",
      " [   1 3549  555 5546]\n",
      " [   2  304 5103 9695]\n",
      " [   3 5425 8762 9054]\n",
      " [   4 8984 8897 7124]]\n",
      "[[0.        5.985731  6.005853  6.235413 ]\n",
      " [0.        5.56559   5.769157  5.8012524]\n",
      " [0.        5.665924  5.6770835 5.9998264]\n",
      " [0.        5.7480392 6.2730412 6.402625 ]\n",
      " [0.        5.45057   5.6836834 5.7167196]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random data\n",
    "d = 64  # dimension\n",
    "nb = 10000  # database size\n",
    "np.random.seed(1234)  # make reproducible\n",
    "data = np.random.random((nb, d)).astype('float32')\n",
    "\n",
    "# Initialize the FAISS index\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance\n",
    "index.add(data)  # add vectors to the index\n",
    "\n",
    "# Search the nearest neighbors\n",
    "k = 4  # we want to see 4 nearest neighbors\n",
    "distances, indices = index.search(data[:5], k)  # actual search\n",
    "print(indices)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56563b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

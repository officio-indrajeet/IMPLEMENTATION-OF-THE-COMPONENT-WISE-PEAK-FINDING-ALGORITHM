{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf76e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathos\n",
      "  Obtaining dependency information for pathos from https://files.pythonhosted.org/packages/f4/7f/cea34872c000d17972dad998575d14656d7c6bcf1a08a8d66d73c1ef2cca/pathos-0.3.2-py3-none-any.whl.metadata\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting ppft>=1.7.6.8 (from pathos)\n",
      "  Obtaining dependency information for ppft>=1.7.6.8 from https://files.pythonhosted.org/packages/ff/fa/5160c7d2fb1d4f2b83cba7a40f0eb4b015b78f6973b7ab6b2e73c233cfdc/ppft-1.7.6.8-py3-none-any.whl.metadata\n",
      "  Downloading ppft-1.7.6.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.8 (from pathos)\n",
      "  Obtaining dependency information for dill>=0.3.8 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.4 (from pathos)\n",
      "  Obtaining dependency information for pox>=0.3.4 from https://files.pythonhosted.org/packages/e1/d7/9e73c32f73da71e8224b4cb861b5db50ebdebcdff14d3e3fb47a63c578b2/pox-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading pox-0.3.4-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.16 (from pathos)\n",
      "  Obtaining dependency information for multiprocess>=0.70.16 from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ppft, pox, dill, multiprocess, pathos\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.14\n",
      "    Uninstalling multiprocess-0.70.14:\n",
      "      Successfully uninstalled multiprocess-0.70.14\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "datasets 2.12.0 requires dill<0.3.7,>=0.3.0, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.8 multiprocess-0.70.16 pathos-0.3.2 pox-0.3.4 ppft-1.7.6.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pathos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a411c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Step 1: Build CCGraph\n",
      "Completed Step 1: Build CCGraph\n",
      "Step 1: Build CCGraph took 0.3247 seconds\n",
      "Starting Step 2: Get Density Dists BB\n",
      "POOL ERROR: Pool not running\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 719 is out of bounds for axis 0 with size 148",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 297\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Measure time taken\u001b[39;00m\n\u001b[1;32m    296\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 297\u001b[0m cpf_model\u001b[38;5;241m.\u001b[39mfit(data_normalized)\n\u001b[1;32m    298\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    300\u001b[0m predicted_labels_cpf \u001b[38;5;241m=\u001b[39m cpf_model\u001b[38;5;241m.\u001b[39mlabels_\n",
      "Cell \u001b[0;32mIn[8], line 225\u001b[0m, in \u001b[0;36mCPFcluster.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Step 2: Get Density Dists BB\u001b[39;00m\n\u001b[1;32m    224\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 225\u001b[0m best_distance, big_brother \u001b[38;5;241m=\u001b[39m get_density_dists_bb(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents, knn_radius, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    226\u001b[0m step2_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 2: Get Density Dists BB took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep2_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 105\u001b[0m, in \u001b[0;36mget_density_dists_bb\u001b[0;34m(X, k, components, knn_radius, n_jobs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         argmin_distance \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39margmin(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m distances]\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(GT_radius\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 105\u001b[0m         cc_big_brother[indx_chunk[i]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(GT_radius[i, :] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][argmin_distance[i]]\n\u001b[1;32m    106\u001b[0m         cc_best_distance[indx_chunk[i]] \u001b[38;5;241m=\u001b[39m distances[i][argmin_distance[i]]\n\u001b[1;32m    108\u001b[0m big_brother[cc_idx] \u001b[38;5;241m=\u001b[39m [cc_idx[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cc_big_brother\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 719 is out of bounds for axis 0 with size 148"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "import gc\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Utility functions\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def density_broad_search_star(a_b):\n",
    "    \"\"\"Wrapper for multiprocessing to call density_broad_search.\"\"\"\n",
    "    try:\n",
    "        return euclidean_distances(a_b[1], a_b[0])\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    print(\"Starting Step 1: Build CCGraph\")\n",
    "    n = X.shape[0]\n",
    "    kdt = NearestNeighbors(n_neighbors=k, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X)\n",
    "    CCmat = kdt.kneighbors_graph(X, mode='distance')\n",
    "    distances, _ = kdt.kneighbors(X)\n",
    "    knn_radius = distances[:, k-1]\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    print(\"Completed Step 1: Build CCGraph\")\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    print(\"Starting Step 2: Get Density Dists BB\")\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    ps = np.zeros((1, 2))\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        kdt = NearestNeighbors(n_neighbors=kcc, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X[cc_idx, :])\n",
    "        distances, neighbors = kdt.kneighbors(X[cc_idx, :])\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        del cc_radius_diff\n",
    "        gc.collect()\n",
    "        cols = cols[unidx]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        search_idx = list(np.setdiff1d(list(range(X[cc_idx, :].shape[0])), rows))\n",
    "        ps = np.vstack((ps, [len(cc_idx), len(search_idx)/len(cc_idx)]))\n",
    "        for indx_chunk in chunks(search_idx, 100):\n",
    "            search_radius = cc_knn_radius[indx_chunk]\n",
    "            GT_radius = cc_knn_radius < search_radius[:, np.newaxis]\n",
    "            if any(np.sum(GT_radius, axis=1) == 0):\n",
    "                max_i = [i for i in range(GT_radius.shape[0]) if np.sum(GT_radius[i, :]) == 0]\n",
    "                if len(max_i) > 1:\n",
    "                    for max_j in max_i[1:len(max_i)]:\n",
    "                        GT_radius[max_j, indx_chunk[max_i[0]]] = True\n",
    "                max_i = max_i[0]\n",
    "                cc_big_brother[indx_chunk[max_i]] = indx_chunk[max_i]\n",
    "                cc_best_distance[indx_chunk[max_i]] = np.inf\n",
    "                del indx_chunk[max_i]\n",
    "                GT_radius = np.delete(GT_radius, max_i, 0)\n",
    "            \n",
    "            GT_distances = ([X[cc_idx[indx_chunk[i]], np.newaxis], X[cc_idx[GT_radius[i, :]], :]] for i in range(len(indx_chunk)))\n",
    "            if (GT_radius.shape[0] > 50):\n",
    "                try:\n",
    "                    pool = Pool(processes=n_jobs)\n",
    "                    distances = list(pool.map(density_broad_search_star, GT_distances))\n",
    "                    argmin_distance = [np.argmin(l) for l in distances]\n",
    "                    pool.terminate()\n",
    "                except Exception as e:\n",
    "                    print(\"POOL ERROR: \" + str(e))\n",
    "                    pool.close()\n",
    "                    pool.terminate()\n",
    "            else:\n",
    "                distances = list(map(density_broad_search_star, list(GT_distances)))\n",
    "                argmin_distance = [np.argmin(l) for l in distances]\n",
    "            \n",
    "            for i in range(GT_radius.shape[0]):\n",
    "                cc_big_brother[indx_chunk[i]] = np.where(GT_radius[i, :] == 1)[0][argmin_distance[i]]\n",
    "                cc_best_distance[indx_chunk[i]] = distances[i][argmin_distance[i]]\n",
    "        \n",
    "        big_brother[cc_idx] = [cc_idx[i] for i in cc_big_brother.astype(int)]\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    print(\"Completed Step 2: Get Density Dists BB\")\n",
    "    return best_distance, big_brother\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    print(\"Starting Step 3: Get Y\")\n",
    "    n = components.shape[0]\n",
    "    y_pred = np.repeat(-1, n)\n",
    "    peaks = []\n",
    "    n_cent = 0\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        tested = []\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        index = np.argsort(cc_idx)\n",
    "        sorted_x = cc_idx[index]\n",
    "        sorted_index = np.searchsorted(sorted_x, big_brother[cc_idx])\n",
    "        cc_big_brother = np.take(index, sorted_index, mode=\"clip\")\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "        peaked = cc_best_distance / cc_knn_radius\n",
    "        peaked[(cc_best_distance == 0) * (cc_knn_radius == 0)] = np.inf\n",
    "        cc_centers = [np.argmax(peaked)]\n",
    "        not_tested[cc_centers[0]] = False\n",
    "        while True:\n",
    "            if np.sum(not_tested) == 0:\n",
    "                break\n",
    "            subset_idx = np.argmax(peaked[not_tested])\n",
    "            prop_cent = np.arange(peaked.shape[0])[not_tested][subset_idx]\n",
    "            tested.append(np.arange(peaked.shape[0])[not_tested][subset_idx])\n",
    "            CCmat_level = CCmat[cc_idx, :][:, cc_idx]\n",
    "            if cc_knn_radius[prop_cent] > max(cc_knn_radius[~not_tested]):\n",
    "                cc_level_set = np.where(cc_knn_radius <= cc_knn_radius[prop_cent])[0]\n",
    "                CCmat_check = CCmat_level[cc_level_set, :][:, cc_level_set]\n",
    "                n_cc, _ = scipy.sparse.csgraph.connected_components(CCmat_check, directed=False, return_labels=True)\n",
    "                if n_cc == 1:\n",
    "                    break\n",
    "            if cc_knn_radius[prop_cent] > 0:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) > e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius < v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            else:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) >= e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius <= v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            _, cc_labels = scipy.sparse.csgraph.connected_components(CCmat_level, directed=False, return_labels=True)\n",
    "            del CCmat_level\n",
    "            gc.collect()\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            prop_cent_comp = cc_labels[np.where(cc_cut_idx == prop_cent)[0]]\n",
    "            if np.isin(prop_cent_comp, center_comp):\n",
    "                if peaked[prop_cent] == min(peaked[cc_centers]):\n",
    "                    cc_centers.append(prop_cent)\n",
    "                    not_tested[prop_cent] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                cc_centers.append(prop_cent)\n",
    "                not_tested[prop_cent] = False\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = cc_big_brother\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    print(\"Completed Step 3: Get Y\")\n",
    "    return y_pred\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise ValueError(\"X must be an n x d numpy array.\")\n",
    "        \n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        # Step 1: Build CCGraph\n",
    "        start_time = time.time()\n",
    "        self.components, self.CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 2: Get Density Dists BB\n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, self.components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 3: Get Y\n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(self.CCmat, self.components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "file_path = 'Live_20210128.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'status_published' to datetime\n",
    "data['status_published'] = pd.to_datetime(data['status_published'])\n",
    "\n",
    "# One-hot encoding for 'status_type'\n",
    "data = pd.get_dummies(data, columns=['status_type'], drop_first=True)\n",
    "\n",
    "# Select relevant features for clustering\n",
    "features = [\n",
    "    'num_reactions', 'num_comments', 'num_shares', \n",
    "    'num_likes', 'num_loves', 'num_wows', 'num_hahas', 'num_sads', 'num_angrys'\n",
    "] + [col for col in data.columns if col.startswith('status_type_')]\n",
    "\n",
    "data_selected = data[features].fillna(0)  # Handle missing values by filling with 0\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_selected)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 3, size=data_normalized.shape[0])  # Assuming 3 clusters for synthetic labels\n",
    "\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    # Removing NaN entries\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)  \n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Protein Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf720e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

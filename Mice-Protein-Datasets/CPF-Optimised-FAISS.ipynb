{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d606ff2a",
   "metadata": {},
   "source": [
    "# Existing CPF Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83209761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DYRK1A_N        3\n",
      "ITSN1_N         3\n",
      "BDNF_N          3\n",
      "NR1_N           3\n",
      "NR2A_N          3\n",
      "pAKT_N          3\n",
      "pBRAF_N         3\n",
      "pCAMKII_N       3\n",
      "pCREB_N         3\n",
      "pELK_N          3\n",
      "pERK_N          3\n",
      "pJNK_N          3\n",
      "PKCA_N          3\n",
      "pMEK_N          3\n",
      "pNR1_N          3\n",
      "pNR2A_N         3\n",
      "pNR2B_N         3\n",
      "pPKCAB_N        3\n",
      "pRSK_N          3\n",
      "AKT_N           3\n",
      "BRAF_N          3\n",
      "CAMKII_N        3\n",
      "CREB_N          3\n",
      "ELK_N          18\n",
      "ERK_N           3\n",
      "GSK3B_N         3\n",
      "JNK_N           3\n",
      "MEK_N           7\n",
      "TRKA_N          3\n",
      "RSK_N           3\n",
      "APP_N           3\n",
      "Bcatenin_N     18\n",
      "SOD1_N          3\n",
      "MTOR_N          3\n",
      "P38_N           3\n",
      "pMTOR_N         3\n",
      "DSCR1_N         3\n",
      "AMPKA_N         3\n",
      "NR2B_N          3\n",
      "pNUMB_N         3\n",
      "RAPTOR_N        3\n",
      "TIAM1_N         3\n",
      "pP70S6_N        3\n",
      "BAD_N         213\n",
      "BCL2_N        285\n",
      "pCFOS_N        75\n",
      "H3AcK18_N     180\n",
      "EGR1_N        210\n",
      "H3MeK4_N      270\n",
      "dtype: int64\n",
      "Starting Step 1: Build CCGraph\n",
      "Completed Step 1: Build CCGraph\n",
      "Step 1: Build CCGraph took 0.1417 seconds\n",
      "Starting Step 2: Get Density Dists BB\n",
      "Completed Step 2: Get Density Dists BB\n",
      "Step 2: Get Density Dists BB took 0.9589 seconds\n",
      "Starting Step 3: Get Y\n",
      "Completed Step 3: Get Y\n",
      "Step 3: Get Y took 1.7974 seconds\n",
      "CPF Clustering:\n",
      "Number of clusters found: 43\n",
      "Total Time taken: 2.90 seconds\n",
      "Adjusted Rand Index (ARI): -0.000877\n",
      "Adjusted Mutual Information (AMI): -0.000764\n",
      "Silhouette Score: 0.106993\n",
      "Davies-Bouldin Index: 1.978943\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# utils module\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def density_broad_search_star(a_b):\n",
    "    \"\"\"Wrapper for multiprocessing to call density_broad_search.\"\"\"\n",
    "    try:\n",
    "        return euclidean_distances(a_b[1], a_b[0])\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    print(\"Starting Step 1: Build CCGraph\")\n",
    "    n = X.shape[0]\n",
    "    kdt = NearestNeighbors(n_neighbors=k, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X)\n",
    "    CCmat = kdt.kneighbors_graph(X, mode='distance')\n",
    "    distances, _ = kdt.kneighbors(X)\n",
    "    knn_radius = distances[:, k-1]\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    print(\"Completed Step 1: Build CCGraph\")\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    print(\"Starting Step 2: Get Density Dists BB\")\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    ps = np.zeros((1, 2))\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        kdt = NearestNeighbors(n_neighbors=kcc, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X[cc_idx, :])\n",
    "        distances, neighbors = kdt.kneighbors(X[cc_idx, :])\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        del cc_radius_diff\n",
    "        gc.collect()\n",
    "        cols = cols[unidx]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        search_idx = list(np.setdiff1d(list(range(X[cc_idx, :].shape[0])), rows))\n",
    "        ps = np.vstack((ps, [len(cc_idx), len(search_idx)/len(cc_idx)]))\n",
    "        for indx_chunk in chunks(search_idx, 100):\n",
    "            search_radius = cc_knn_radius[indx_chunk]\n",
    "            GT_radius = cc_knn_radius < search_radius[:, np.newaxis]\n",
    "            if any(np.sum(GT_radius, axis=1) == 0):\n",
    "                max_i = [i for i in range(GT_radius.shape[0]) if np.sum(GT_radius[i, :]) == 0]\n",
    "                if len(max_i) > 1:\n",
    "                    for max_j in max_i[1:len(max_i)]:\n",
    "                        GT_radius[max_j, indx_chunk[max_i[0]]] = True\n",
    "                max_i = max_i[0]\n",
    "                cc_big_brother[indx_chunk[max_i]] = indx_chunk[max_i]\n",
    "                cc_best_distance[indx_chunk[max_i]] = np.inf\n",
    "                del indx_chunk[max_i]\n",
    "                GT_radius = np.delete(GT_radius, max_i, 0)\n",
    "            \n",
    "            GT_distances = ([X[cc_idx[indx_chunk[i]], np.newaxis], X[cc_idx[GT_radius[i, :]], :]] for i in range(len(indx_chunk)))\n",
    "            if (GT_radius.shape[0] > 50):\n",
    "                try:\n",
    "                    pool = mp.Pool(processes=n_jobs)\n",
    "                    N = 25\n",
    "                    distances = []\n",
    "                    i = 0\n",
    "                    while True:\n",
    "                        distance_comp = pool.map(density_broad_search_star, itertools.islice(GT_distances, N))\n",
    "                        if distance_comp:\n",
    "                            distances.append(distance_comp)\n",
    "                            i += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    distances = [dis_pair for dis_list in distances for dis_pair in dis_list]\n",
    "                    argmin_distance = [np.argmin(l) for l in distances]\n",
    "                    pool.terminate()\n",
    "                except Exception as e:\n",
    "                    print(\"POOL ERROR: \" + str(e))\n",
    "                    pool.close()\n",
    "                    pool.terminate()\n",
    "            else:\n",
    "                distances = list(map(density_broad_search_star, list(GT_distances)))\n",
    "                argmin_distance = [np.argmin(l) for l in distances]\n",
    "            \n",
    "            for i in range(GT_radius.shape[0]):\n",
    "                cc_big_brother[indx_chunk[i]] = np.where(GT_radius[i, :] == 1)[0][argmin_distance[i]]\n",
    "                cc_best_distance[indx_chunk[i]] = distances[i][argmin_distance[i]]\n",
    "        \n",
    "        big_brother[cc_idx] = [cc_idx[i] for i in cc_big_brother.astype(int)]\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    print(\"Completed Step 2: Get Density Dists BB\")\n",
    "    return best_distance, big_brother\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    print(\"Starting Step 3: Get Y\")\n",
    "    n = components.shape[0]\n",
    "    y_pred = np.repeat(-1, n)\n",
    "    peaks = []\n",
    "    n_cent = 0\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        tested = []\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        index = np.argsort(cc_idx)\n",
    "        sorted_x = cc_idx[index]\n",
    "        sorted_index = np.searchsorted(sorted_x, big_brother[cc_idx])\n",
    "        cc_big_brother = np.take(index, sorted_index, mode=\"clip\")\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "        peaked = cc_best_distance / cc_knn_radius\n",
    "        peaked[(cc_best_distance == 0) * (cc_knn_radius == 0)] = np.inf\n",
    "        cc_centers = [np.argmax(peaked)]\n",
    "        not_tested[cc_centers[0]] = False\n",
    "        while True:\n",
    "            if np.sum(not_tested) == 0:\n",
    "                break\n",
    "            subset_idx = np.argmax(peaked[not_tested])\n",
    "            prop_cent = np.arange(peaked.shape[0])[not_tested][subset_idx]\n",
    "            tested.append(np.arange(peaked.shape[0])[not_tested][subset_idx])\n",
    "            CCmat_level = CCmat[cc_idx, :][:, cc_idx]\n",
    "            if cc_knn_radius[prop_cent] > max(cc_knn_radius[~not_tested]):\n",
    "                cc_level_set = np.where(cc_knn_radius <= cc_knn_radius[prop_cent])[0]\n",
    "                CCmat_check = CCmat_level[cc_level_set, :][:, cc_level_set]\n",
    "                n_cc, _ = scipy.sparse.csgraph.connected_components(CCmat_check, directed=False, return_labels=True)\n",
    "                if n_cc == 1:\n",
    "                    break\n",
    "            if cc_knn_radius[prop_cent] > 0:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) > e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius < v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            else:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) >= e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius <= v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            _, cc_labels = scipy.sparse.csgraph.connected_components(CCmat_level, directed=False, return_labels=True)\n",
    "            del CCmat_level\n",
    "            gc.collect()\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            gc.collect()\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            prop_cent_comp = cc_labels[np.where(cc_cut_idx == prop_cent)[0]]\n",
    "            if np.isin(prop_cent_comp, center_comp):\n",
    "                if peaked[prop_cent] == min(peaked[cc_centers]):\n",
    "                    cc_centers.append(prop_cent)\n",
    "                    not_tested[prop_cent] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                cc_centers.append(prop_cent)\n",
    "                not_tested[prop_cent] = False\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = cc_big_brother\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    print(\"Completed Step 3: Get Y\")\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise ValueError(\"X must be an n x d numpy array.\")\n",
    "        \n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        # Step 1: Build CCGraph\n",
    "        start_time = time.time()\n",
    "        self.components, self.CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 2: Get Density Dists BB\n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, self.components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 3: Get Y\n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(self.CCmat, self.components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data_Cortex_Nuclear.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop non-numeric columns and the target variable 'class' from the features\n",
    "features = data.drop(columns=['MouseID', 'Genotype', 'Treatment', 'Behavior', 'class'])\n",
    "\n",
    "# Check for missing values again\n",
    "missing_values = features.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 8, size=data_normalized.shape[0])  # Assuming 8 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    # Removing NaN entries\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Protein Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442bda70",
   "metadata": {},
   "source": [
    "# Optimised CPF Algorithm ( build_CCgraph Using FAISS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa6af89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DYRK1A_N        3\n",
      "ITSN1_N         3\n",
      "BDNF_N          3\n",
      "NR1_N           3\n",
      "NR2A_N          3\n",
      "pAKT_N          3\n",
      "pBRAF_N         3\n",
      "pCAMKII_N       3\n",
      "pCREB_N         3\n",
      "pELK_N          3\n",
      "pERK_N          3\n",
      "pJNK_N          3\n",
      "PKCA_N          3\n",
      "pMEK_N          3\n",
      "pNR1_N          3\n",
      "pNR2A_N         3\n",
      "pNR2B_N         3\n",
      "pPKCAB_N        3\n",
      "pRSK_N          3\n",
      "AKT_N           3\n",
      "BRAF_N          3\n",
      "CAMKII_N        3\n",
      "CREB_N          3\n",
      "ELK_N          18\n",
      "ERK_N           3\n",
      "GSK3B_N         3\n",
      "JNK_N           3\n",
      "MEK_N           7\n",
      "TRKA_N          3\n",
      "RSK_N           3\n",
      "APP_N           3\n",
      "Bcatenin_N     18\n",
      "SOD1_N          3\n",
      "MTOR_N          3\n",
      "P38_N           3\n",
      "pMTOR_N         3\n",
      "DSCR1_N         3\n",
      "AMPKA_N         3\n",
      "NR2B_N          3\n",
      "pNUMB_N         3\n",
      "RAPTOR_N        3\n",
      "TIAM1_N         3\n",
      "pP70S6_N        3\n",
      "BAD_N         213\n",
      "BCL2_N        285\n",
      "pCFOS_N        75\n",
      "H3AcK18_N     180\n",
      "EGR1_N        210\n",
      "H3MeK4_N      270\n",
      "dtype: int64\n",
      "Starting Step 1: Build CCGraph\n",
      "Completed Step 1: Build CCGraph\n",
      "Step 1: Build CCGraph took 0.0335 seconds\n",
      "Starting Step 2: Get Density Dists BB\n",
      "Completed Step 2: Get Density Dists BB\n",
      "Step 2: Get Density Dists BB took 0.6231 seconds\n",
      "Starting Step 3: Get Y\n",
      "Completed Step 3: Get Y\n",
      "Step 3: Get Y took 0.6713 seconds\n",
      "CPF Clustering:\n",
      "Number of clusters found: 27\n",
      "Total Time taken: 1.33 seconds\n",
      "Adjusted Rand Index (ARI): 0.001684\n",
      "Adjusted Mutual Information (AMI): 0.004135\n",
      "Silhouette Score: 0.085715\n",
      "Davies-Bouldin Index: 1.710065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import faiss\n",
    "\n",
    "# utils module\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def density_broad_search_star(a_b):\n",
    "    \"\"\"Wrapper for multiprocessing to call density_broad_search.\"\"\"\n",
    "    try:\n",
    "        return euclidean_distances(a_b[1], a_b[0])\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    print(\"Starting Step 1: Build CCGraph\")\n",
    "    n = X.shape[0]\n",
    "    X = np.ascontiguousarray(X, dtype=np.float32)  # Ensure the array is C-contiguous and of type float32\n",
    "    index = faiss.IndexFlatL2(X.shape[1])\n",
    "    faiss.normalize_L2(X)\n",
    "    index.add(X)\n",
    "    distances, indices = index.search(X, k)\n",
    "    knn_radius = distances[:, -1]\n",
    "    CCmat = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in indices[i]:\n",
    "            CCmat[i, j] = distances[i, np.where(indices[i] == j)]\n",
    "    CCmat = scipy.sparse.csr_matrix(CCmat)\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    print(\"Completed Step 1: Build CCGraph\")\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    print(\"Starting Step 2: Get Density Dists BB\")\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    ps = np.zeros((1, 2))\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        kdt = NearestNeighbors(n_neighbors=kcc, metric='euclidean', n_jobs=n_jobs, algorithm='kd_tree').fit(X[cc_idx, :])\n",
    "        distances, neighbors = kdt.kneighbors(X[cc_idx, :])\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        del cc_radius_diff\n",
    "        gc.collect()\n",
    "        cols = cols[unidx]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        search_idx = list(np.setdiff1d(list(range(X[cc_idx, :].shape[0])), rows))\n",
    "        ps = np.vstack((ps, [len(cc_idx), len(search_idx)/len(cc_idx)]))\n",
    "        for indx_chunk in chunks(search_idx, 100):\n",
    "            search_radius = cc_knn_radius[indx_chunk]\n",
    "            GT_radius = cc_knn_radius < search_radius[:, np.newaxis]\n",
    "            if any(np.sum(GT_radius, axis=1) == 0):\n",
    "                max_i = [i for i in range(GT_radius.shape[0]) if np.sum(GT_radius[i, :]) == 0]\n",
    "                if len(max_i) > 1:\n",
    "                    for max_j in max_i[1:len(max_i)]:\n",
    "                        GT_radius[max_j, indx_chunk[max_i[0]]] = True\n",
    "                max_i = max_i[0]\n",
    "                cc_big_brother[indx_chunk[max_i]] = indx_chunk[max_i]\n",
    "                cc_best_distance[indx_chunk[max_i]] = np.inf\n",
    "                del indx_chunk[max_i]\n",
    "                GT_radius = np.delete(GT_radius, max_i, 0)\n",
    "            \n",
    "            GT_distances = ([X[cc_idx[indx_chunk[i]], np.newaxis], X[cc_idx[GT_radius[i, :]], :]] for i in range(len(indx_chunk)))\n",
    "            if (GT_radius.shape[0] > 50):\n",
    "                try:\n",
    "                    pool = mp.Pool(processes=n_jobs)\n",
    "                    N = 25\n",
    "                    distances = []\n",
    "                    i = 0\n",
    "                    while True:\n",
    "                        distance_comp = pool.map(density_broad_search_star, itertools.islice(GT_distances, N))\n",
    "                        if distance_comp:\n",
    "                            distances.append(distance_comp)\n",
    "                            i += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    distances = [dis_pair for dis_list in distances for dis_pair in dis_list]\n",
    "                    argmin_distance = [np.argmin(l) for l in distances]\n",
    "                    pool.terminate()\n",
    "                except Exception as e:\n",
    "                    print(\"POOL ERROR: \" + str(e))\n",
    "                    pool.close()\n",
    "                    pool.terminate()\n",
    "            else:\n",
    "                distances = list(map(density_broad_search_star, list(GT_distances)))\n",
    "                argmin_distance = [np.argmin(l) for l in distances]\n",
    "            \n",
    "            for i in range(GT_radius.shape[0]):\n",
    "                cc_big_brother[indx_chunk[i]] = np.where(GT_radius[i, :] == 1)[0][argmin_distance[i]]\n",
    "                cc_best_distance[indx_chunk[i]] = distances[i][argmin_distance[i]]\n",
    "        \n",
    "        big_brother[cc_idx] = [cc_idx[i] for i in cc_big_brother.astype(int)]\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    print(\"Completed Step 2: Get Density Dists BB\")\n",
    "    return best_distance, big_brother\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    print(\"Starting Step 3: Get Y\")\n",
    "    n = components.shape[0]\n",
    "    y_pred = np.repeat(-1, n)\n",
    "    peaks = []\n",
    "    n_cent = 0\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        tested = []\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        index = np.argsort(cc_idx)\n",
    "        sorted_x = cc_idx[index]\n",
    "        sorted_index = np.searchsorted(sorted_x, big_brother[cc_idx])\n",
    "        cc_big_brother = np.take(index, sorted_index, mode=\"clip\")\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "        peaked = cc_best_distance / cc_knn_radius\n",
    "        peaked[(cc_best_distance == 0) * (cc_knn_radius == 0)] = np.inf\n",
    "        cc_centers = [np.argmax(peaked)]\n",
    "        not_tested[cc_centers[0]] = False\n",
    "        while True:\n",
    "            if np.sum(not_tested) == 0:\n",
    "                break\n",
    "            subset_idx = np.argmax(peaked[not_tested])\n",
    "            prop_cent = np.arange(peaked.shape[0])[not_tested][subset_idx]\n",
    "            tested.append(np.arange(peaked.shape[0])[not_tested][subset_idx])\n",
    "            CCmat_level = CCmat[cc_idx, :][:, cc_idx]\n",
    "            if cc_knn_radius[prop_cent] > max(cc_knn_radius[~not_tested]):\n",
    "                cc_level_set = np.where(cc_knn_radius <= cc_knn_radius[prop_cent])[0]\n",
    "                CCmat_check = CCmat_level[cc_level_set, :][:, cc_level_set]\n",
    "                n_cc, _ = scipy.sparse.csgraph.connected_components(CCmat_check, directed=False, return_labels=True)\n",
    "                if n_cc == 1:\n",
    "                    break\n",
    "            if cc_knn_radius[prop_cent] > 0:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) > e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius < v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            else:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) >= e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius <= v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            _, cc_labels = scipy.sparse.csgraph.connected_components(CCmat_level, directed=False, return_labels=True)\n",
    "            del CCmat_level\n",
    "            gc.collect()\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            prop_cent_comp = cc_labels[np.where(cc_cut_idx == prop_cent)[0]]\n",
    "            if np.isin(prop_cent_comp, center_comp):\n",
    "                if peaked[prop_cent] == min(peaked[cc_centers]):\n",
    "                    cc_centers.append(prop_cent)\n",
    "                    not_tested[prop_cent] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                cc_centers.append(prop_cent)\n",
    "                not_tested[prop_cent] = False\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = cc_big_brother\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    print(\"Completed Step 3: Get Y\")\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise ValueError(\"X must be an n x d numpy array.\")\n",
    "        \n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        # Step 1: Build CCGraph\n",
    "        start_time = time.time()\n",
    "        self.components, self.CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 2: Get Density Dists BB\n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, self.components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 3: Get Y\n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(self.CCmat, self.components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data_Cortex_Nuclear.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop non-numeric columns and the target variable 'class' from the features\n",
    "features = data.drop(columns=['MouseID', 'Genotype', 'Treatment', 'Behavior', 'class'])\n",
    "\n",
    "# Check for missing values again\n",
    "missing_values = features.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 8, size=data_normalized.shape[0])  # Assuming 8 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    # Removing NaN entries\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)  # Use all available CPU cores\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Protein Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54187024",
   "metadata": {},
   "source": [
    "# Sample FAISS Code for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25edf382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 5325 7124 5754]\n",
      " [   1 3549  555 5546]\n",
      " [   2  304 5103 9695]\n",
      " [   3 5425 8762 9054]\n",
      " [   4 8984 8897 7124]]\n",
      "[[0.        5.985731  6.005853  6.235413 ]\n",
      " [0.        5.56559   5.769157  5.8012524]\n",
      " [0.        5.665924  5.6770835 5.9998264]\n",
      " [0.        5.7480392 6.2730412 6.402625 ]\n",
      " [0.        5.45057   5.6836834 5.7167196]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Generate some random data\n",
    "d = 64  # dimension\n",
    "nb = 10000  # database size\n",
    "np.random.seed(1234)  # make reproducible\n",
    "data = np.random.random((nb, d)).astype('float32')\n",
    "\n",
    "# Initialize the FAISS index\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance\n",
    "index.add(data)  # add vectors to the index\n",
    "\n",
    "# Search the nearest neighbors\n",
    "k = 4  # we want to see 4 nearest neighbors\n",
    "distances, indices = index.search(data[:5], k)  # actual search\n",
    "print(indices)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f466a",
   "metadata": {},
   "source": [
    "# Further Optimising CPF Algorithm ( get_density_dists_bb with FAISS )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63520de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DYRK1A_N        3\n",
      "ITSN1_N         3\n",
      "BDNF_N          3\n",
      "NR1_N           3\n",
      "NR2A_N          3\n",
      "pAKT_N          3\n",
      "pBRAF_N         3\n",
      "pCAMKII_N       3\n",
      "pCREB_N         3\n",
      "pELK_N          3\n",
      "pERK_N          3\n",
      "pJNK_N          3\n",
      "PKCA_N          3\n",
      "pMEK_N          3\n",
      "pNR1_N          3\n",
      "pNR2A_N         3\n",
      "pNR2B_N         3\n",
      "pPKCAB_N        3\n",
      "pRSK_N          3\n",
      "AKT_N           3\n",
      "BRAF_N          3\n",
      "CAMKII_N        3\n",
      "CREB_N          3\n",
      "ELK_N          18\n",
      "ERK_N           3\n",
      "GSK3B_N         3\n",
      "JNK_N           3\n",
      "MEK_N           7\n",
      "TRKA_N          3\n",
      "RSK_N           3\n",
      "APP_N           3\n",
      "Bcatenin_N     18\n",
      "SOD1_N          3\n",
      "MTOR_N          3\n",
      "P38_N           3\n",
      "pMTOR_N         3\n",
      "DSCR1_N         3\n",
      "AMPKA_N         3\n",
      "NR2B_N          3\n",
      "pNUMB_N         3\n",
      "RAPTOR_N        3\n",
      "TIAM1_N         3\n",
      "pP70S6_N        3\n",
      "BAD_N         213\n",
      "BCL2_N        285\n",
      "pCFOS_N        75\n",
      "H3AcK18_N     180\n",
      "EGR1_N        210\n",
      "H3MeK4_N      270\n",
      "dtype: int64\n",
      "Starting Step 1: Build CCGraph\n",
      "Completed Step 1: Build CCGraph\n",
      "Step 1: Build CCGraph took 0.0377 seconds\n",
      "Starting Step 2: Get Density Dists BB\n",
      "Completed Step 2: Get Density Dists BB\n",
      "Step 2: Get Density Dists BB took 0.0097 seconds\n",
      "Starting Step 3: Get Y\n",
      "Completed Step 3: Get Y\n",
      "Step 3: Get Y took 1.4121 seconds\n",
      "CPF Clustering:\n",
      "Number of clusters found: 42\n",
      "Total Time taken: 1.46 seconds\n",
      "Adjusted Rand Index (ARI): -0.000586\n",
      "Adjusted Mutual Information (AMI): 0.001285\n",
      "Silhouette Score: -0.321589\n",
      "Davies-Bouldin Index: 2.227751\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import faiss\n",
    "\n",
    "# utils module\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def density_broad_search_star(a_b):\n",
    "    \"\"\"Wrapper for multiprocessing to call density_broad_search.\"\"\"\n",
    "    try:\n",
    "        return euclidean_distances(a_b[1], a_b[0])\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    print(\"Starting Step 1: Build CCGraph\")\n",
    "    n = X.shape[0]\n",
    "    X = np.ascontiguousarray(X, dtype=np.float32)  # Ensure the array is C-contiguous and of type float32\n",
    "    index = faiss.IndexFlatL2(X.shape[1])\n",
    "    faiss.normalize_L2(X)\n",
    "    index.add(X)\n",
    "    distances, indices = index.search(X, k)\n",
    "    knn_radius = distances[:, -1]\n",
    "    CCmat = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in indices[i]:\n",
    "            CCmat[i, j] = distances[i, np.where(indices[i] == j)]\n",
    "    CCmat = scipy.sparse.csr_matrix(CCmat)\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    print(\"Completed Step 1: Build CCGraph\")\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    print(\"Starting Step 2: Get Density Dists BB\")\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    \n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        \n",
    "        # Using FAISS for neighbor search\n",
    "        index = faiss.IndexFlatL2(X.shape[1])\n",
    "        X_cc = X[cc_idx]\n",
    "        X_cc = np.ascontiguousarray(X_cc,dtype=np.float32)  # Ensure the array is C-contiguous\n",
    "        faiss.normalize_L2(X_cc)\n",
    "        index.add(X_cc)\n",
    "        distances, neighbors = index.search(X_cc, kcc)\n",
    "        \n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        cols = cols[unidx]\n",
    "        \n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        \n",
    "        search_idx = list(np.setdiff1d(list(range(X[cc_idx, :].shape[0])), rows))\n",
    "        for indx_chunk in chunks(search_idx, 100):\n",
    "            search_radius = cc_knn_radius[indx_chunk]\n",
    "            GT_radius = cc_knn_radius < search_radius[:, np.newaxis]\n",
    "            if any(np.sum(GT_radius, axis=1) == 0):\n",
    "                max_i = [i for i in range(GT_radius.shape[0]) if np.sum(GT_radius[i, :]) == 0]\n",
    "                if len(max_i) > 1:\n",
    "                    for max_j in max_i[1:len(max_i)]:\n",
    "                        GT_radius[max_j, indx_chunk[max_i[0]]] = True\n",
    "                max_i = max_i[0]\n",
    "                cc_big_brother[indx_chunk[max_i]] = indx_chunk[max_i]\n",
    "                cc_best_distance[indx_chunk[max_i]] = np.inf\n",
    "                del indx_chunk[max_i]\n",
    "                GT_radius = np.delete(GT_radius, max_i, 0)\n",
    "            \n",
    "            GT_distances = ([X[cc_idx[indx_chunk[i]], np.newaxis], X[cc_idx[GT_radius[i, :]], :]] for i in range(len(indx_chunk)))\n",
    "            if (GT_radius.shape[0] > 50):\n",
    "                try:\n",
    "                    pool = mp.Pool(processes=n_jobs)\n",
    "                    distances = pool.map(density_broad_search_star, GT_distances)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "                except Exception as e:\n",
    "                    print(\"POOL ERROR: \" + str(e))\n",
    "                    pool.terminate()\n",
    "            else:\n",
    "                distances = list(map(density_broad_search_star, GT_distances))\n",
    "            \n",
    "            argmin_distance = [np.argmin(l) for l in distances]\n",
    "            for i in range(GT_radius.shape[0]):\n",
    "                cc_big_brother[indx_chunk[i]] = np.where(GT_radius[i, :] == 1)[0][argmin_distance[i]]\n",
    "                cc_best_distance[indx_chunk[i]] = distances[i][argmin_distance[i]]\n",
    "        \n",
    "        big_brother[cc_idx] = cc_big_brother.astype(int)\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    print(\"Completed Step 2: Get Density Dists BB\")\n",
    "    return best_distance, big_brother\n",
    "\n",
    "\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    print(\"Starting Step 3: Get Y\")\n",
    "    n = components.shape[0]\n",
    "    y_pred = np.repeat(-1, n)\n",
    "    peaks = []\n",
    "    n_cent = 0\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        tested = []\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        index = np.argsort(cc_idx)\n",
    "        sorted_x = cc_idx[index]\n",
    "        sorted_index = np.searchsorted(sorted_x, big_brother[cc_idx])\n",
    "        cc_big_brother = np.take(index, sorted_index, mode=\"clip\")\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "        peaked = cc_best_distance / cc_knn_radius\n",
    "        peaked[(cc_best_distance == 0) * (cc_knn_radius == 0)] = np.inf\n",
    "        cc_centers = [np.argmax(peaked)]\n",
    "        not_tested[cc_centers[0]] = False\n",
    "        while True:\n",
    "            if np.sum(not_tested) == 0:\n",
    "                break\n",
    "            subset_idx = np.argmax(peaked[not_tested])\n",
    "            prop_cent = np.arange(peaked.shape[0])[not_tested][subset_idx]\n",
    "            tested.append(np.arange(peaked.shape[0])[not_tested][subset_idx])\n",
    "            CCmat_level = CCmat[cc_idx, :][:, cc_idx]\n",
    "            if cc_knn_radius[prop_cent] > max(cc_knn_radius[~not_tested]):\n",
    "                cc_level_set = np.where(cc_knn_radius <= cc_knn_radius[prop_cent])[0]\n",
    "                CCmat_check = CCmat_level[cc_level_set, :][:, cc_level_set]\n",
    "                n_cc, _ = scipy.sparse.csgraph.connected_components(CCmat_check, directed=False, return_labels=True)\n",
    "                if n_cc == 1:\n",
    "                    break\n",
    "            if cc_knn_radius[prop_cent] > 0:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) > e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius < v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            else:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) >= e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius <= v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            _, cc_labels = scipy.sparse.csgraph.connected_components(CCmat_level, directed=False, return_labels=True)\n",
    "            del CCmat_level\n",
    "            gc.collect()\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            prop_cent_comp = cc_labels[np.where(cc_cut_idx == prop_cent)[0]]\n",
    "            if np.isin(prop_cent_comp, center_comp):\n",
    "                if peaked[prop_cent] == min(peaked[cc_centers]):\n",
    "                    cc_centers.append(prop_cent)\n",
    "                    not_tested[prop_cent] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                cc_centers.append(prop_cent)\n",
    "                not_tested[prop_cent] = False\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = cc_big_brother\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    print(\"Completed Step 3: Get Y\")\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise ValueError(\"X must be an n x d numpy array.\")\n",
    "        \n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        # Step 1: Build CCGraph\n",
    "        start_time = time.time()\n",
    "        self.components, self.CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 2: Get Density Dists BB\n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, self.components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 3: Get Y\n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(self.CCmat, self.components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data_Cortex_Nuclear.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop non-numeric columns and the target variable 'class' from the features\n",
    "features = data.drop(columns=['MouseID', 'Genotype', 'Treatment', 'Behavior', 'class'])\n",
    "\n",
    "# Check for missing values again\n",
    "missing_values = features.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 8, size=data_normalized.shape[0])  # Assuming 8 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    # Removing NaN entries\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)  # Use all available CPU cores\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Protein Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e76bb2",
   "metadata": {},
   "source": [
    "# Further Optimising CPF Algorithm (get_Y with FAISS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ea1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DYRK1A_N        3\n",
      "ITSN1_N         3\n",
      "BDNF_N          3\n",
      "NR1_N           3\n",
      "NR2A_N          3\n",
      "pAKT_N          3\n",
      "pBRAF_N         3\n",
      "pCAMKII_N       3\n",
      "pCREB_N         3\n",
      "pELK_N          3\n",
      "pERK_N          3\n",
      "pJNK_N          3\n",
      "PKCA_N          3\n",
      "pMEK_N          3\n",
      "pNR1_N          3\n",
      "pNR2A_N         3\n",
      "pNR2B_N         3\n",
      "pPKCAB_N        3\n",
      "pRSK_N          3\n",
      "AKT_N           3\n",
      "BRAF_N          3\n",
      "CAMKII_N        3\n",
      "CREB_N          3\n",
      "ELK_N          18\n",
      "ERK_N           3\n",
      "GSK3B_N         3\n",
      "JNK_N           3\n",
      "MEK_N           7\n",
      "TRKA_N          3\n",
      "RSK_N           3\n",
      "APP_N           3\n",
      "Bcatenin_N     18\n",
      "SOD1_N          3\n",
      "MTOR_N          3\n",
      "P38_N           3\n",
      "pMTOR_N         3\n",
      "DSCR1_N         3\n",
      "AMPKA_N         3\n",
      "NR2B_N          3\n",
      "pNUMB_N         3\n",
      "RAPTOR_N        3\n",
      "TIAM1_N         3\n",
      "pP70S6_N        3\n",
      "BAD_N         213\n",
      "BCL2_N        285\n",
      "pCFOS_N        75\n",
      "H3AcK18_N     180\n",
      "EGR1_N        210\n",
      "H3MeK4_N      270\n",
      "dtype: int64\n",
      "Starting Step 1: Build CCGraph\n",
      "Completed Step 1: Build CCGraph\n",
      "Step 1: Build CCGraph took 0.0375 seconds\n",
      "Starting Step 2: Get Density Dists BB\n",
      "Completed Step 2: Get Density Dists BB\n",
      "Step 2: Get Density Dists BB took 0.0114 seconds\n",
      "Starting Step 3: Get Y\n",
      "Completed Step 3: Get Y\n",
      "Step 3: Get Y took 0.0210 seconds\n",
      "CPF Clustering:\n",
      "Number of clusters found: 35\n",
      "Total Time taken: 0.07 seconds\n",
      "Adjusted Rand Index (ARI): -0.000620\n",
      "Adjusted Mutual Information (AMI): 0.001193\n",
      "Silhouette Score: -0.298625\n",
      "Davies-Bouldin Index: 2.145985\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse\n",
    "import time\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import faiss\n",
    "\n",
    "# utils module\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def density_broad_search_star(a_b):\n",
    "    \"\"\"Wrapper for multiprocessing to call density_broad_search.\"\"\"\n",
    "    try:\n",
    "        return euclidean_distances(a_b[1], a_b[0])\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "# CPF functions\n",
    "def build_CCgraph(X, k, cutoff, n_jobs):\n",
    "    print(\"Starting Step 1: Build CCGraph\")\n",
    "    n = X.shape[0]\n",
    "    X = np.ascontiguousarray(X, dtype=np.float32)  # Ensure the array is C-contiguous and of type float32\n",
    "    index = faiss.IndexFlatL2(X.shape[1])\n",
    "    faiss.normalize_L2(X)\n",
    "    index.add(X)\n",
    "    distances, indices = index.search(X, k)\n",
    "    knn_radius = distances[:, -1]\n",
    "    CCmat = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in indices[i]:\n",
    "            CCmat[i, j] = distances[i, np.where(indices[i] == j)]\n",
    "    CCmat = scipy.sparse.csr_matrix(CCmat)\n",
    "    CCmat = CCmat.minimum(CCmat.T)\n",
    "    _, components = scipy.sparse.csgraph.connected_components(CCmat, directed=False, return_labels=True)\n",
    "    comp_labs, comp_count = np.unique(components, return_counts=True)\n",
    "    outlier_components = comp_labs[comp_count <= cutoff]\n",
    "    nanidx = np.in1d(components, outlier_components)\n",
    "    components = components.astype(float)\n",
    "    if sum(nanidx) > 0:\n",
    "        components[nanidx] = np.nan\n",
    "    print(\"Completed Step 1: Build CCGraph\")\n",
    "    return components, CCmat, knn_radius\n",
    "\n",
    "def get_density_dists_bb(X, k, components, knn_radius, n_jobs):\n",
    "    print(\"Starting Step 2: Get Density Dists BB\")\n",
    "    best_distance = np.empty((X.shape[0]))\n",
    "    best_distance[:] = np.nan\n",
    "    big_brother = np.empty((X.shape[0]))\n",
    "    big_brother[:] = np.nan\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "    \n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        kcc = min(k, nc-1)\n",
    "        \n",
    "        # Using FAISS for neighbor search\n",
    "        index = faiss.IndexFlatL2(X.shape[1])\n",
    "        X_cc = X[cc_idx]\n",
    "        X_cc = np.ascontiguousarray(X_cc,dtype=np.float32)  # Ensure the array is C-contiguous\n",
    "        faiss.normalize_L2(X_cc)\n",
    "        index.add(X_cc)\n",
    "        distances, neighbors = index.search(X_cc, kcc)\n",
    "        \n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = np.empty((nc))\n",
    "        cc_big_brother = np.empty((nc))\n",
    "        cc_radius_diff = cc_knn_radius[:, np.newaxis] - cc_knn_radius[neighbors]\n",
    "        rows, cols = np.where(cc_radius_diff > 0)\n",
    "        rows, unidx = np.unique(rows, return_index=True)\n",
    "        cols = cols[unidx]\n",
    "        \n",
    "        cc_best_distance[rows] = distances[rows, cols]\n",
    "        cc_big_brother[rows] = neighbors[rows, cols]\n",
    "        \n",
    "        search_idx = list(np.setdiff1d(list(range(X[cc_idx, :].shape[0])), rows))\n",
    "        for indx_chunk in chunks(search_idx, 100):\n",
    "            search_radius = cc_knn_radius[indx_chunk]\n",
    "            GT_radius = cc_knn_radius < search_radius[:, np.newaxis]\n",
    "            if any(np.sum(GT_radius, axis=1) == 0):\n",
    "                max_i = [i for i in range(GT_radius.shape[0]) if np.sum(GT_radius[i, :]) == 0]\n",
    "                if len(max_i) > 1:\n",
    "                    for max_j in max_i[1:len(max_i)]:\n",
    "                        GT_radius[max_j, indx_chunk[max_i[0]]] = True\n",
    "                max_i = max_i[0]\n",
    "                cc_big_brother[indx_chunk[max_i]] = indx_chunk[max_i]\n",
    "                cc_best_distance[indx_chunk[max_i]] = np.inf\n",
    "                del indx_chunk[max_i]\n",
    "                GT_radius = np.delete(GT_radius, max_i, 0)\n",
    "            \n",
    "            GT_distances = ([X[cc_idx[indx_chunk[i]], np.newaxis], X[cc_idx[GT_radius[i, :]], :]] for i in range(len(indx_chunk)))\n",
    "            if (GT_radius.shape[0] > 50):\n",
    "                try:\n",
    "                    pool = mp.Pool(processes=n_jobs)\n",
    "                    distances = pool.map(density_broad_search_star, GT_distances)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "                except Exception as e:\n",
    "                    print(\"POOL ERROR: \" + str(e))\n",
    "                    pool.terminate()\n",
    "            else:\n",
    "                distances = list(map(density_broad_search_star, GT_distances))\n",
    "            \n",
    "            argmin_distance = [np.argmin(l) for l in distances]\n",
    "            for i in range(GT_radius.shape[0]):\n",
    "                cc_big_brother[indx_chunk[i]] = np.where(GT_radius[i, :] == 1)[0][argmin_distance[i]]\n",
    "                cc_best_distance[indx_chunk[i]] = distances[i][argmin_distance[i]]\n",
    "        \n",
    "        big_brother[cc_idx] = cc_big_brother.astype(int)\n",
    "        best_distance[cc_idx] = cc_best_distance\n",
    "    \n",
    "    print(\"Completed Step 2: Get Density Dists BB\")\n",
    "    return best_distance, big_brother\n",
    "\n",
    "\n",
    "def get_y(CCmat, components, knn_radius, best_distance, big_brother, rho, alpha, d):\n",
    "    print(\"Starting Step 3: Get Y\")\n",
    "    n = components.shape[0]\n",
    "    y_pred = np.repeat(-1, n)\n",
    "    peaks = []\n",
    "    n_cent = 0\n",
    "    comps = np.unique((components[~np.isnan(components)])).astype(int)\n",
    "\n",
    "    for cc in comps:\n",
    "        cc_idx = np.where(components == cc)[0]\n",
    "        nc = len(cc_idx)\n",
    "        tested = []\n",
    "        cc_knn_radius = knn_radius[cc_idx]\n",
    "        cc_best_distance = best_distance[cc_idx]\n",
    "        index = np.argsort(cc_idx)\n",
    "        sorted_x = cc_idx[index]\n",
    "        sorted_index = np.searchsorted(sorted_x, big_brother[cc_idx])\n",
    "        cc_big_brother = np.take(index, sorted_index, mode=\"clip\")\n",
    "        not_tested = np.ones(nc, dtype=bool)\n",
    "        peaked = cc_best_distance / cc_knn_radius\n",
    "        peaked[(cc_best_distance == 0) * (cc_knn_radius == 0)] = np.inf\n",
    "        cc_centers = [np.argmax(peaked)]\n",
    "        not_tested[cc_centers[0]] = False\n",
    "\n",
    "        while True:\n",
    "            if np.sum(not_tested) == 0:\n",
    "                break\n",
    "            subset_idx = np.argmax(peaked[not_tested])\n",
    "            prop_cent = np.arange(peaked.shape[0])[not_tested][subset_idx]\n",
    "            tested.append(np.arange(peaked.shape[0])[not_tested][subset_idx])\n",
    "            CCmat_level = CCmat[cc_idx, :][:, cc_idx]\n",
    "            if cc_knn_radius[prop_cent] > max(cc_knn_radius[~not_tested]):\n",
    "                cc_level_set = np.where(cc_knn_radius <= cc_knn_radius[prop_cent])[0]\n",
    "                CCmat_check = CCmat_level[cc_level_set, :][:, cc_level_set]\n",
    "                n_cc, _ = scipy.sparse.csgraph.connected_components(CCmat_check, directed=False, return_labels=True)\n",
    "                if n_cc == 1:\n",
    "                    break\n",
    "            if cc_knn_radius[prop_cent] > 0:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) > e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius < v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "            else:\n",
    "                v_cutoff = cc_knn_radius[prop_cent] / (rho ** (1 / d))\n",
    "                e_cutoff = cc_knn_radius[prop_cent] / alpha\n",
    "                e_mask = np.abs(CCmat_level.data) >= e_cutoff\n",
    "                CCmat_level.data[e_mask] = 0\n",
    "                CCmat_level.eliminate_zeros()\n",
    "                cc_cut_idx = np.where(cc_knn_radius <= v_cutoff)[0]\n",
    "                CCmat_level = CCmat_level[cc_cut_idx, :][:, cc_cut_idx]\n",
    "\n",
    "            # Using FAISS for connected components\n",
    "            faiss_index = faiss.IndexFlatL2(CCmat_level.shape[1])\n",
    "            X_cc = CCmat_level.toarray().astype('float32')  # Ensure the array is float32\n",
    "            faiss.normalize_L2(X_cc)\n",
    "            faiss_index.add(X_cc)\n",
    "            _, cc_labels = faiss_index.search(X_cc, 1)\n",
    "\n",
    "            center_comp = cc_labels[np.isin(cc_cut_idx, cc_centers)]\n",
    "            prop_cent_comp = cc_labels[np.where(cc_cut_idx == prop_cent)[0]]\n",
    "            if np.isin(prop_cent_comp, center_comp):\n",
    "                if peaked[prop_cent] == min(peaked[cc_centers]):\n",
    "                    cc_centers.append(prop_cent)\n",
    "                    not_tested[prop_cent] = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                cc_centers.append(prop_cent)\n",
    "                not_tested[prop_cent] = False\n",
    "\n",
    "        cc_centers = np.array(cc_centers)\n",
    "        peaks.extend(cc_idx[cc_centers])\n",
    "        BBTree = np.zeros((nc, 2))\n",
    "        BBTree[:, 0] = range(nc)\n",
    "        BBTree[:, 1] = cc_big_brother\n",
    "        BBTree[cc_centers, 1] = cc_centers\n",
    "        BBTree = BBTree.astype(int)\n",
    "        Clustmat = scipy.sparse.csr_matrix((np.ones((nc)), (BBTree[:, 0], BBTree[:, 1])), shape=(nc, nc))\n",
    "        n_clusts, cc_y_pred = scipy.sparse.csgraph.connected_components(Clustmat, directed=True, return_labels=True)\n",
    "        cc_y_pred += n_cent\n",
    "        n_cent += n_clusts\n",
    "        y_pred[cc_idx] = cc_y_pred\n",
    "\n",
    "    print(\"Completed Step 3: Get Y\")\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class CPFcluster:\n",
    "    def __init__(self, k, rho=0.4, alpha=1, n_jobs=1, remove_duplicates=False, cutoff=1):\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.n_jobs = n_jobs\n",
    "        self.remove_duplicates = remove_duplicates\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "    def fit(self, X):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise ValueError(\"X must be an n x d numpy array.\")\n",
    "        \n",
    "        if self.remove_duplicates:\n",
    "            X = np.unique(X, axis=0)\n",
    "        \n",
    "        n, d = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k cannot be larger than n.\")\n",
    "        \n",
    "        # Step 1: Build CCGraph\n",
    "        start_time = time.time()\n",
    "        self.components, self.CCmat, knn_radius = build_CCgraph(X, self.k, self.cutoff, self.n_jobs)\n",
    "        step1_time = time.time() - start_time\n",
    "        print(f\"Step 1: Build CCGraph took {step1_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 2: Get Density Dists BB\n",
    "        start_time = time.time()\n",
    "        best_distance, big_brother = get_density_dists_bb(X, self.k, self.components, knn_radius, self.n_jobs)\n",
    "        step2_time = time.time() - start_time\n",
    "        print(f\"Step 2: Get Density Dists BB took {step2_time:.4f} seconds\")\n",
    "        \n",
    "        # Step 3: Get Y\n",
    "        start_time = time.time()\n",
    "        self.labels_ = get_y(self.CCmat, self.components, knn_radius, best_distance, big_brother, self.rho, self.alpha, d)\n",
    "        step3_time = time.time() - start_time\n",
    "        print(f\"Step 3: Get Y took {step3_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data_Cortex_Nuclear.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop non-numeric columns and the target variable 'class' from the features\n",
    "features = data.drop(columns=['MouseID', 'Genotype', 'Treatment', 'Behavior', 'class'])\n",
    "\n",
    "# Check for missing values again\n",
    "missing_values = features.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Add synthetic labels for ARI and AMI calculations\n",
    "np.random.seed(42)\n",
    "true_labels = np.random.randint(0, 8, size=data_normalized.shape[0])  # Assuming 8 clusters for synthetic labels\n",
    "\n",
    "# Function to plot PCA results\n",
    "def plot_pca_2d(data, labels, title, file_name):\n",
    "    pca = PCA(n_components=2)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=labels, palette='tab10', s=50, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print clustering metrics\n",
    "def print_clustering_metrics(true_labels, predicted_labels):\n",
    "    # Removing NaN entries\n",
    "    valid_indices = ~np.isnan(predicted_labels)\n",
    "    true_labels = true_labels[valid_indices]\n",
    "    predicted_labels = predicted_labels[valid_indices]\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    silhouette_avg = silhouette_score(data_normalized[valid_indices], predicted_labels)\n",
    "    davies_bouldin = davies_bouldin_score(data_normalized[valid_indices], predicted_labels)\n",
    "    \n",
    "    print(f'Adjusted Rand Index (ARI): {ari:.6f}')\n",
    "    print(f'Adjusted Mutual Information (AMI): {ami:.6f}')\n",
    "    print(f'Silhouette Score: {silhouette_avg:.6f}')\n",
    "    print(f'Davies-Bouldin Index: {davies_bouldin:.6f}')\n",
    "\n",
    "# Use the CPF cluster\n",
    "cpf_model = CPFcluster(k=10, rho=0.4, n_jobs=1)  # Use all available CPU cores\n",
    "\n",
    "# Measure time taken\n",
    "start_time = time.time()\n",
    "cpf_model.fit(data_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_labels_cpf = cpf_model.labels_\n",
    "\n",
    "# Calculate number of clusters found\n",
    "num_clusters = len(np.unique(predicted_labels_cpf[~np.isnan(predicted_labels_cpf)]))\n",
    "\n",
    "print(\"CPF Clustering:\")\n",
    "print(f\"Number of clusters found: {num_clusters}\")\n",
    "print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Calculate and print clustering metrics\n",
    "print_clustering_metrics(true_labels, predicted_labels_cpf)\n",
    "\n",
    "# Plot PCA results\n",
    "# plot_pca_2d(data_normalized, predicted_labels_cpf, 'PCA of Protein Data with CPF Clusters', 'pca_clusters_CPF.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de7f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
